{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e590f1f",
   "metadata": {},
   "source": [
    "**MCF training on  EyeQ dataset for eye fundus quality assessment**\n",
    "\n",
    "We suppose that the preprocessing done using the EyeQ preprocess section has already been done, this notebook already reaches the preprocessed data to train the MCF Network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cc808",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99516a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageCms\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872c3e7",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ed23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eyeq dataset on cloud\n",
    "csv_path='/workspace/data/data retinax/data_kaggle/RFiMD/Training_Set/Training_Set/RFMiD_Training_Labels.csv',\n",
    "image_folder='/workspace/data/data retinax/data_kaggle/RFiMD/Training_Set/Training_Set/Training',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806a159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eyeQ_excel(data_dir, list_file, n_class=3):\n",
    "    image_names = []\n",
    "    labels = []\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(np.array(range(n_class)))\n",
    "    df_tmp = pd.read_csv(list_file)\n",
    "    img_num = len(df_tmp)\n",
    "\n",
    "    for idx in range(img_num):\n",
    "        image_name = df_tmp[\"image\"][idx]\n",
    "        image_names.append(os.path.join(data_dir, image_name[:-5] + '.png'))\n",
    "\n",
    "        label = lb.transform([int(df_tmp[\"quality\"][idx])])\n",
    "        labels.append(label)\n",
    "\n",
    "    return image_names, labels\n",
    "\n",
    "\n",
    "class DatasetGenerator(Dataset):\n",
    "    def __init__(self, data_dir, list_file, transform1=None, transform2=None, n_class=3, set_name='train'):\n",
    "\n",
    "        image_names, labels = load_eyeQ_excel(data_dir, list_file, n_class=3)\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.n_class = n_class\n",
    "        self.transform1 = transform1\n",
    "        self.transform2 = transform2\n",
    "        self.set_name = set_name\n",
    "\n",
    "        srgb_profile = ImageCms.createProfile(\"sRGB\")\n",
    "        lab_profile = ImageCms.createProfile(\"LAB\")\n",
    "        self.rgb2lab_transform = ImageCms.buildTransformFromOpenProfiles(srgb_profile, lab_profile, \"RGB\", \"LAB\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_names[index]\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "\n",
    "        if self.transform1 is not None:\n",
    "            image = self.transform1(image)\n",
    "\n",
    "        img_hsv = image.convert(\"HSV\")\n",
    "        img_lab = ImageCms.applyTransform(image, self.rgb2lab_transform)\n",
    "\n",
    "        img_rgb = np.asarray(image).astype('float32')\n",
    "        img_hsv = np.asarray(img_hsv).astype('float32')\n",
    "        img_lab = np.asarray(img_lab).astype('float32')\n",
    "\n",
    "        if self.transform2 is not None:\n",
    "            img_rgb = self.transform2(img_rgb)\n",
    "            img_hsv = self.transform2(img_hsv)\n",
    "            img_lab = self.transform2(img_lab)\n",
    "\n",
    "        if self.set_name == 'train':\n",
    "            label = self.labels[index]\n",
    "            return torch.FloatTensor(img_rgb), torch.FloatTensor(img_hsv), torch.FloatTensor(img_lab), torch.FloatTensor(label)\n",
    "        else:\n",
    "            return torch.FloatTensor(img_rgb), torch.FloatTensor(img_hsv), torch.FloatTensor(img_lab)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ee581",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ae1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(datanpGT, datanpPRED, target_names):\n",
    "\n",
    "    n_class = len(target_names)\n",
    "    argmaxPRED = np.argmax(datanpPRED, axis=1)\n",
    "    F1_metric = np.zeros([n_class, 1])\n",
    "    tn = np.zeros([n_class, 1])\n",
    "    fp = np.zeros([n_class, 1])\n",
    "    fn = np.zeros([n_class, 1])\n",
    "    tp = np.zeros([n_class, 1])\n",
    "\n",
    "    Accuracy_score = accuracy_score(datanpGT, argmaxPRED)\n",
    "    ROC_curve = {}\n",
    "    mAUC = 0\n",
    "\n",
    "    for i in range(n_class):\n",
    "        tmp_label = datanpGT == i\n",
    "        tmp_pred = argmaxPRED == i\n",
    "        F1_metric[i] = f1_score(tmp_label, tmp_pred)\n",
    "        tn[i], fp[i], fn[i], tp[i] = confusion_matrix(tmp_label, tmp_pred).ravel()\n",
    "        outAUROC = roc_auc_score(tmp_label, datanpPRED[:, i])\n",
    "\n",
    "        mAUC = mAUC + outAUROC\n",
    "        [roc_fpr, roc_tpr, roc_thresholds] = roc_curve(tmp_label, datanpPRED[:, i])\n",
    "\n",
    "        ROC_curve.update({'ROC_fpr_'+str(i): roc_fpr,\n",
    "                          'ROC_tpr_' + str(i): roc_tpr,\n",
    "                          'ROC_T_' + str(i): roc_thresholds,\n",
    "                          'AUC_' + str(i): outAUROC})\n",
    "\n",
    "    mPrecision = sum(tp) / sum(tp + fp)\n",
    "    mRecall = sum(tp) / sum(tp + fn)\n",
    "    output = {\n",
    "        'class_name': target_names,\n",
    "        'F1': F1_metric,\n",
    "        'AUC': mAUC / 3,\n",
    "        'Accuracy': Accuracy_score,\n",
    "\n",
    "        'Sensitivity': tp / (tp + fn),\n",
    "        'Precision': tp / (tp + fp),\n",
    "        'Specificity': tn / (fp + tn),\n",
    "        'ROC_curve': ROC_curve,\n",
    "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
    "\n",
    "        'micro-Precision': mPrecision,\n",
    "        'micro-Sensitivity': mRecall,\n",
    "        'micro-Specificity': sum(tn) / sum(fp + tn),\n",
    "        'micro-F1': 2*mPrecision * mRecall / (mPrecision + mRecall),\n",
    "    }\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c64fc7",
   "metadata": {},
   "source": [
    "## display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c696e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "from collections import deque\n",
    "from datetime import timedelta\n",
    "from math import ceil\n",
    "from sys import stderr\n",
    "from time import time\n",
    "\n",
    "\n",
    "__version__ = '1.4'\n",
    "\n",
    "HIDE_CURSOR = '\\x1b[?25l'\n",
    "SHOW_CURSOR = '\\x1b[?25h'\n",
    "\n",
    "\n",
    "class Infinite(object):\n",
    "    file = stderr\n",
    "    sma_window = 10         # Simple Moving Average window\n",
    "    check_tty = True\n",
    "    hide_cursor = True\n",
    "\n",
    "    def __init__(self, message='', **kwargs):\n",
    "        self.index = 0\n",
    "        self.start_ts = time()\n",
    "        self.avg = 0\n",
    "        self._ts = self.start_ts\n",
    "        self._xput = deque(maxlen=self.sma_window)\n",
    "        for key, val in kwargs.items():\n",
    "            setattr(self, key, val)\n",
    "\n",
    "        self._width = 0\n",
    "        self.message = message\n",
    "\n",
    "        if self.file and self.is_tty():\n",
    "            if self.hide_cursor:\n",
    "                print(HIDE_CURSOR, end='', file=self.file)\n",
    "            print(self.message, end='', file=self.file)\n",
    "            self.file.flush()\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key.startswith('_'):\n",
    "            return None\n",
    "        return getattr(self, key, None)\n",
    "\n",
    "    @property\n",
    "    def elapsed(self):\n",
    "        return int(time() - self.start_ts)\n",
    "\n",
    "    @property\n",
    "    def elapsed_td(self):\n",
    "        return timedelta(seconds=self.elapsed)\n",
    "\n",
    "    def update_avg(self, n, dt):\n",
    "        if n > 0:\n",
    "            self._xput.append(dt / n)\n",
    "            self.avg = sum(self._xput) / len(self._xput)\n",
    "\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def start(self):\n",
    "        pass\n",
    "\n",
    "    def clearln(self):\n",
    "        if self.file and self.is_tty():\n",
    "            print('\\r\\x1b[K', end='', file=self.file)\n",
    "\n",
    "    def write(self, s):\n",
    "        if self.file and self.is_tty():\n",
    "            line = self.message + s.ljust(self._width)\n",
    "            print('\\r' + line, end='', file=self.file)\n",
    "            self._width = max(self._width, len(s))\n",
    "            self.file.flush()\n",
    "\n",
    "    def writeln(self, line):\n",
    "        if self.file and self.is_tty():\n",
    "            self.clearln()\n",
    "            print(line, end='', file=self.file)\n",
    "            self.file.flush()\n",
    "\n",
    "    def finish(self):\n",
    "        if self.file and self.is_tty():\n",
    "            print(file=self.file)\n",
    "            if self.hide_cursor:\n",
    "                print(SHOW_CURSOR, end='', file=self.file)\n",
    "\n",
    "    def is_tty(self):\n",
    "        return self.file.isatty() if self.check_tty else True\n",
    "\n",
    "    def next(self, n=1):\n",
    "        now = time()\n",
    "        dt = now - self._ts\n",
    "        self.update_avg(n, dt)\n",
    "        self._ts = now\n",
    "        self.index = self.index + n\n",
    "        self.update()\n",
    "\n",
    "    def iter(self, it):\n",
    "        with self:\n",
    "            for x in it:\n",
    "                yield x\n",
    "                self.next()\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.finish()\n",
    "\n",
    "\n",
    "class Progress(Infinite):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Progress, self).__init__(*args, **kwargs)\n",
    "        self.max = kwargs.get('max', 100)\n",
    "\n",
    "    @property\n",
    "    def eta(self):\n",
    "        return int(ceil(self.avg * self.remaining))\n",
    "\n",
    "    @property\n",
    "    def eta_td(self):\n",
    "        return timedelta(seconds=self.eta)\n",
    "\n",
    "    @property\n",
    "    def percent(self):\n",
    "        return self.progress * 100\n",
    "\n",
    "    @property\n",
    "    def progress(self):\n",
    "        return min(1, self.index / self.max)\n",
    "\n",
    "    @property\n",
    "    def remaining(self):\n",
    "        return max(self.max - self.index, 0)\n",
    "\n",
    "    def start(self):\n",
    "        self.update()\n",
    "\n",
    "    def goto(self, index):\n",
    "        incr = index - self.index\n",
    "        self.next(incr)\n",
    "\n",
    "    def iter(self, it):\n",
    "        try:\n",
    "            self.max = len(it)\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        with self:\n",
    "            for x in it:\n",
    "                yield x\n",
    "                self.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from . import Infinite, Progress\n",
    "\n",
    "\n",
    "class Counter(Infinite):\n",
    "    def update(self):\n",
    "        self.write(str(self.index))\n",
    "\n",
    "\n",
    "class Countdown(Progress):\n",
    "    def update(self):\n",
    "        self.write(str(self.remaining))\n",
    "\n",
    "\n",
    "class Stack(Progress):\n",
    "    phases = (' ', '▁', '▂', '▃', '▄', '▅', '▆', '▇', '█')\n",
    "\n",
    "    def update(self):\n",
    "        nphases = len(self.phases)\n",
    "        i = min(nphases - 1, int(self.progress * nphases))\n",
    "        self.write(self.phases[i])\n",
    "\n",
    "\n",
    "class Pie(Stack):\n",
    "    phases = ('○', '◔', '◑', '◕', '●')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from . import Infinite\n",
    "\n",
    "\n",
    "class Spinner(Infinite):\n",
    "    phases = ('-', '\\\\', '|', '/')\n",
    "    hide_cursor = True\n",
    "\n",
    "    def update(self):\n",
    "        i = self.index % len(self.phases)\n",
    "        self.write(self.phases[i])\n",
    "\n",
    "\n",
    "class PieSpinner(Spinner):\n",
    "    phases = ['◷', '◶', '◵', '◴']\n",
    "\n",
    "\n",
    "class MoonSpinner(Spinner):\n",
    "    phases = ['◑', '◒', '◐', '◓']\n",
    "\n",
    "\n",
    "class LineSpinner(Spinner):\n",
    "    phases = ['⎺', '⎻', '⎼', '⎽', '⎼', '⎻']\n",
    "\n",
    "\n",
    "class PixelSpinner(Spinner):\n",
    "    phases = ['⣾', '⣷', '⣯', '⣟', '⡿', '⢿', '⣻', '⣽']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a4ce05",
   "metadata": {},
   "source": [
    "## network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7b7a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121_v0(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_class):\n",
    "        super(DenseNet121_v0, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=False)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, n_class),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class dense121_mcs(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super(dense121_mcs, self).__init__()\n",
    "\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=False)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "\n",
    "        A_model = DenseNet121_v0(n_class=n_class)\n",
    "        self.featureA = A_model\n",
    "        self.classA = A_model.densenet121.features\n",
    "\n",
    "        B_model = DenseNet121_v0(n_class=n_class)\n",
    "        self.featureB = B_model\n",
    "        self.classB = B_model.densenet121.features\n",
    "\n",
    "        C_model = DenseNet121_v0(n_class=n_class)\n",
    "        self.featureC = C_model\n",
    "        self.classC = C_model.densenet121.features\n",
    "\n",
    "        self.combine1 = nn.Sequential(\n",
    "            nn.Linear(n_class * 4, n_class),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.combine2 = nn.Sequential(\n",
    "            nn.Linear(num_ftrs * 3, n_class),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        x1 = self.featureA(x)\n",
    "        y1 = self.featureB(y)\n",
    "        z1 = self.featureC(z)\n",
    "        x2 = self.classA(x)\n",
    "        x2 = F.relu(x2, inplace=True)\n",
    "        x2 = F.adaptive_avg_pool2d(x2, (1, 1)).view(x2.size(0), -1)\n",
    "        y2 = self.classB(y)\n",
    "        y2 = F.relu(y2, inplace=True)\n",
    "        y2 = F.adaptive_avg_pool2d(y2, (1, 1)).view(y2.size(0), -1)\n",
    "        z2 = self.classC(z)\n",
    "        z2 = F.relu(z2, inplace=True)\n",
    "        z2 = F.adaptive_avg_pool2d(z2, (1, 1)).view(z2.size(0), -1)\n",
    "\n",
    "        combine = torch.cat((x2.view(x2.size(0), -1),\n",
    "                             y2.view(y2.size(0), -1),\n",
    "                             z2.view(z2.size(0), -1)), 1)\n",
    "        combine = self.combine2(combine)\n",
    "\n",
    "        combine3 = torch.cat((x1.view(x1.size(0), -1),\n",
    "                              y1.view(y1.size(0), -1),\n",
    "                              z1.view(z1.size(0), -1),\n",
    "                              combine.view(combine.size(0), -1)), 1)\n",
    "\n",
    "        combine3 = self.combine1(combine3)\n",
    "\n",
    "        return x1, y1, z1, combine, combine3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd4e49",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f0238f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_loader, model, epoch, optimizer, criterion, args):\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    loss_w =args.loss_w\n",
    "\n",
    "    iters_per_epoch = len(train_loader)\n",
    "    bar = Bar('Processing {} Epoch -> {} / {}'.format('train', epoch+1, args.epochs), max=iters_per_epoch)\n",
    "    bar.check_tty = False\n",
    "\n",
    "    for step, (imagesA, imagesB, imagesC, labels) in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        imagesA = imagesA.cuda()\n",
    "        imagesB = imagesB.cuda()\n",
    "        imagesC = imagesC.cuda()\n",
    "\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        out_A, out_B, out_C, out_F, combine = model(imagesA, imagesB, imagesC)\n",
    "\n",
    "        loss_x = criterion(out_A, labels)\n",
    "        loss_y = criterion(out_B, labels)\n",
    "        loss_z = criterion(out_C, labels)\n",
    "        loss_c = criterion(out_F, labels)\n",
    "        loss_f = criterion(combine, labels)\n",
    "\n",
    "        lossValue = loss_w[0]*loss_x+loss_w[1]*loss_y+loss_w[2]*loss_z+loss_w[3]*loss_c+loss_w[4]*loss_f\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lossValue.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        epoch_loss += lossValue.item()\n",
    "        end_time = time.time()\n",
    "        batch_time = end_time - start_time\n",
    "        # plot progress\n",
    "        bar_str = '{} / {} | Time: {batch_time:.2f} mins | Loss: {loss:.4f} '\n",
    "        bar.suffix = bar_str.format(step+1, iters_per_epoch, batch_time=batch_time*(iters_per_epoch-step)/60,\n",
    "                                    loss=lossValue.item())\n",
    "        bar.next()\n",
    "\n",
    "    epoch_loss = epoch_loss / iters_per_epoch\n",
    "\n",
    "    bar.finish()\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validation_step(val_loader, model, criterion):\n",
    "\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    iters_per_epoch = len(val_loader)\n",
    "    bar = Bar('Processing {}'.format('validation'), max=iters_per_epoch)\n",
    "\n",
    "    for step, (imagesA, imagesB, imagesC, labels) in enumerate(val_loader):\n",
    "        start_time = time.time()\n",
    "\n",
    "        imagesA = imagesA.cuda()\n",
    "        imagesB = imagesB.cuda()\n",
    "        imagesC = imagesC.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        _, _, _, _, outputs = model(imagesA, imagesB, imagesC)\n",
    "        with torch.no_grad():\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time = end_time - start_time\n",
    "        bar_str = '{} / {} | Time: {batch_time:.2f} mins'\n",
    "        bar.suffix = bar_str.format(step + 1, len(val_loader), batch_time=batch_time * (iters_per_epoch - step) / 60)\n",
    "        bar.next()\n",
    "\n",
    "    epoch_loss = epoch_loss / iters_per_epoch\n",
    "    bar.finish()\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def save_output(label_test_file, dataPRED, args, save_file):\n",
    "    label_list = args.label_idx\n",
    "    n_class = len(label_list)\n",
    "    datanpPRED = np.squeeze(dataPRED.cpu().numpy())\n",
    "    df_tmp = pd.read_csv(label_test_file)\n",
    "    image_names = df_tmp[\"image\"].tolist()\n",
    "\n",
    "    result = {label_list[i]: datanpPRED[:, i] for i in range(n_class)}\n",
    "    result['image_name'] = image_names\n",
    "    out_df = pd.DataFrame(result)\n",
    "\n",
    "    name_older = ['image_name']\n",
    "    for i in range(n_class):\n",
    "        name_older.append(label_list[i])\n",
    "    out_df.to_csv(save_file, columns=name_older)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031a060",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33c04da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_dir MODEL_DIR]\n",
      "                             [--pre_model PRE_MODEL] [--save_model SAVE_MODEL]\n",
      "                             [--crop_size CROP_SIZE] [--label_idx LABEL_IDX]\n",
      "                             [--n_classes N_CLASSES] [--epochs EPOCHS]\n",
      "                             [--batch-size BATCH_SIZE] [--lr LR]\n",
      "                             [--loss_w LOSS_W]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/abelsalmona/Library/Jupyter/runtime/kernel-v39d9b32ee1dbf39f8bc1bf3065b4ebc9b0c7dcd70.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abelsalmona/Library/Python/3.13/lib/python/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "data_root = '../Kaggle_DR_dataset/'\n",
    "\n",
    "# Setting parameters\n",
    "parser = argparse.ArgumentParser(description='EyeQ_dense121')\n",
    "parser.add_argument('--model_dir', type=str, default='./result/')\n",
    "parser.add_argument('--pre_model', type=str, default='DenseNet121_v3_v1')\n",
    "parser.add_argument('--save_model', type=str, default='DenseNet121_v3_v1')\n",
    "\n",
    "parser.add_argument('--crop_size', type=int, default=224)\n",
    "parser.add_argument('--label_idx', type=list, default=['Good', 'Usable', 'Reject'])\n",
    "\n",
    "parser.add_argument('--n_classes', type=int, default=3)\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', default=20, type=int)\n",
    "parser.add_argument('--batch-size', default=4, type=int)\n",
    "parser.add_argument('--lr', default=0.01, type=float)\n",
    "parser.add_argument('--loss_w', default=[0.1, 0.1, 0.1, 0.1, 0.6], type=list)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Images Labels\n",
    "train_images_dir = data_root + '/train'\n",
    "label_train_file = '../data/Label_EyeQ_train.csv'\n",
    "test_images_dir = data_root + '/test'\n",
    "label_test_file = '../data/Label_EyeQ_test.csv'\n",
    "\n",
    "save_file_name = args.model_dir + args.save_model + '.csv'\n",
    "\n",
    "best_metric = np.inf\n",
    "best_iter = 0\n",
    "# options\n",
    "cudnn.benchmark = True\n",
    "\n",
    "model = dense121_mcs(n_class=args.n_classes)\n",
    "\n",
    "if args.pre_model is not None:\n",
    "    loaded_model = torch.load(os.path.join(args.model_dir, args.pre_model + '.tar'))\n",
    "    model.load_state_dict(loaded_model['state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "print('Total params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "\n",
    "transform_list1 = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(degrees=(-180, +180)),\n",
    "    ])\n",
    "\n",
    "transformList2 = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transform_list_val1 = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "    ])\n",
    "\n",
    "data_train = DatasetGenerator(data_dir=train_images_dir, list_file=label_train_file, transform1=transform_list1,\n",
    "                              transform2=transformList2, n_class=args.n_classes, set_name='train')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data_train, batch_size=args.batch_size,\n",
    "                                               shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "data_test = DatasetGenerator(data_dir=test_images_dir, list_file=label_test_file, transform1=transform_list_val1,\n",
    "                             transform2=transformList2, n_class=args.n_classes, set_name='test')\n",
    "test_loader = torch.utils.data.DataLoader(dataset=data_test, batch_size=args.batch_size,\n",
    "                                          shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "# # Train and val\n",
    "# for epoch in range(0, args.epochs):\n",
    "#     _ = train_step(train_loader, model, epoch, optimizer, criterion, args)\n",
    "#     validation_loss = validation_step(val_loader, model, criterion)\n",
    "#     print('Current Loss: {}| Best Loss: {} at epoch: {}'.format(validation_loss, best_metric, best_iter))\n",
    "#\n",
    "#     # save model\n",
    "#     if best_metric > validation_loss:\n",
    "#         best_metric = validation_loss\n",
    "#         best_iter = epoch\n",
    "#         model_save_file = os.path.join(args.save_dir, args.save_model + '.tar')\n",
    "#         if not os.path.exists(args.save_dir):\n",
    "#             os.makedirs(args.save_dir)\n",
    "#         torch.save({'state_dict': model.state_dict(), 'best_loss': best_metric}, model_save_file)\n",
    "#         print('Model saved to %s' % model_save_file)\n",
    "\n",
    "\n",
    "# Testing\n",
    "outPRED_mcs = torch.FloatTensor().cuda()\n",
    "model.eval()\n",
    "iters_per_epoch = len(test_loader)\n",
    "bar = Bar('Processing {}'.format('inference'), max=len(test_loader))\n",
    "bar.check_tty = False\n",
    "for epochID, (imagesA, imagesB, imagesC) in enumerate(test_loader):\n",
    "    imagesA = imagesA.cuda()\n",
    "    imagesB = imagesB.cuda()\n",
    "    imagesC = imagesC.cuda()\n",
    "\n",
    "    begin_time = time.time()\n",
    "    _, _, _, _, result_mcs = model(imagesA, imagesB, imagesC)\n",
    "    outPRED_mcs = torch.cat((outPRED_mcs, result_mcs.data), 0)\n",
    "    batch_time = time.time() - begin_time\n",
    "    bar.suffix = '{} / {} | Time: {batch_time:.4f}'.format(epochID + 1, len(test_loader),\n",
    "                                                           batch_time=batch_time * (iters_per_epoch - epochID) / 60)\n",
    "    bar.next()\n",
    "bar.finish()\n",
    "\n",
    "# save result into excel:\n",
    "save_output(label_test_file, outPRED_mcs, args, save_file=save_file_name)\n",
    "\n",
    "\n",
    "# evaluation:\n",
    "df_gt = pd.read_csv(label_test_file)\n",
    "img_list = df_gt[\"image\"].tolist()\n",
    "GT_QA_list = np.array(df_gt[\"quality\"].tolist())\n",
    "img_num = len(img_list)\n",
    "label_list = [\"Good\", \"Usable\", \"Reject\"]\n",
    "\n",
    "df_tmp = pd.read_csv(save_file_name)\n",
    "predict_tmp = np.zeros([img_num, 3])\n",
    "for idx in range(3):\n",
    "    predict_tmp[:, idx] = np.array(df_tmp[label_list[idx]].tolist())\n",
    "tmp_report = compute_metric(GT_QA_list, predict_tmp, target_names=label_list)\n",
    "\n",
    "print(' Accuracy: ' + str(\"{:0.4f}\".format(np.mean(tmp_report['Accuracy']))) +\n",
    "      ' Precision: ' + str(\"{:0.4f}\".format(np.mean(tmp_report['Precision']))) +\n",
    "      ' Sensitivity: ' + str(\"{:0.4f}\".format(np.mean(tmp_report['Sensitivity']))) +\n",
    "      ' F1: ' + str(\"{:0.4f}\".format(np.mean(tmp_report['F1']))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
